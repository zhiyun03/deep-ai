# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-02-01 16:47+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"

#: ../../source/models/model_abilities/chat.rst:5
msgid "Chat & Generate"
msgstr "聊天 & 生成"

#: ../../source/models/model_abilities/chat.rst:7
msgid "Learn how to chat with LLMs in Xinference."
msgstr "学习如何在 Xinference 中与 LLM 聊天。"

#: ../../source/models/model_abilities/chat.rst:10
msgid "Introduction"
msgstr "介绍"

#: ../../source/models/model_abilities/chat.rst:12
msgid ""
"Models equipped with ``chat`` or ``generate`` abilities are frequently "
"referred to as large language models (LLM) or text generation models. "
"These models are designed to respond with text outputs to the inputs they"
" receive, commonly known as \"prompts\". Typically, one can direct these "
"models using specific instructions or by providing concrete examples "
"illustrating how to accomplish a task."
msgstr "具备 ``chat`` 或 ``generate`` 能力的模型通常被称为大型语言模型（LLM）或文本生成模型。"
"这些模型旨在根据接收到的输入以文本输出方式进行回应，通常被称为“提示”。"
"一般来说，可以通过特定指令或提供具体示例来引导这些模型完成任务。"

#: ../../source/models/model_abilities/chat.rst:17
msgid ""
"Models with ``generate`` capacities are typically pre-trained large "
"language models. On the other hand, models equipped with ``chat`` "
"capabilities are finely-tuned and aligned LLMs, optimized for dialogues "
"use case. In most cases, models ending with \"chat\" (e.g. "
"``llama-2-chat``, ``qwen-chat``, etc) are identified as having ``chat`` "
"capabilities."
msgstr "具备 ``generate`` 能力的模型通常是预训练的大型语言模型。"
"另一方面，配备 ``chat`` 功能的模型是经过精调和对齐的 LLM（Language Model），专为对话场景进行优化。"
"在大多数情况下，以“chat”结尾的模型（例如 ``llama-2-chat``，``qwen-chat`` 等）则具有 ``chat`` 功能。"

#: ../../source/models/model_abilities/chat.rst:22
msgid ""
"The Chat API and Generate API offer two distinct approaches for "
"interacting with LLMs:"
msgstr "Chat API 和 Generate API 提供了两种不同的与 LLMs 进行交互的方法："

#: ../../source/models/model_abilities/chat.rst:24
msgid ""
"The Chat API (like OpenAI's `Chat Completion API "
"<https://platform.openai.com/docs/api-reference/chat/create>`__) can "
"conduct multi-turn conversations."
msgstr "Chat API（类似于 OpenAI 的 `Chat Completion API <https://platform.openai.com/docs/api-reference/chat/create>`__）可以进行多轮对话。"

#: ../../source/models/model_abilities/chat.rst:27
msgid ""
"The Generate API (like OpenAI's legacy `Completions API "
"<https://platform.openai.com/docs/api-reference/completions/create>`__) "
"allows you to generate text based on a text prompt."
msgstr "Generate API（类似于 OpenAI 的 `Completions API <https://platform.openai.com/docs/api-reference/completions/create>`__ ）允许您根据文本提示生成文本。"

#: ../../source/models/model_abilities/chat.rst:34
msgid "MODEL ABILITY"
msgstr "模型能力"

#: ../../source/models/model_abilities/chat.rst:35
msgid "API ENDPOINT"
msgstr "API 端点"

#: ../../source/models/model_abilities/chat.rst:36
msgid "OpenAI-compatible ENDPOINT"
msgstr "OpenAI 兼容端点"

#: ../../source/models/model_abilities/chat.rst:38
msgid "chat"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:39
#: ../../source/models/model_abilities/chat.rst:56
msgid "Chat API"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:40
msgid "/v1/chat/completions"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:42
msgid "generate"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:43
#: ../../source/models/model_abilities/chat.rst:157
msgid "Generate API"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:44
msgid "/v1/completions"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:48
msgid "Supported models"
msgstr "支持的模型列表"

#: ../../source/models/model_abilities/chat.rst:50
msgid ""
"You can examine the abilities of all the :ref:`builtin LLM models in "
"Xinference <models_llm_index>`."
msgstr "你可以查看所有 :ref:`Xinference 中内置的 LLM 模型的能力 <models_llm_index>`。"

#: ../../source/models/model_abilities/chat.rst:53
msgid "Quickstart"
msgstr "快速入门"

#: ../../source/models/model_abilities/chat.rst:58
msgid ""
"The Chat API mimics OpenAI's `Chat Completion API "
"<https://platform.openai.com/docs/api-reference/chat/create>`__. We can "
"try Chat API out either via cURL, OpenAI Client, or Xinference's python "
"client:"
msgstr "尝试使用 cURL、OpenAI Client 或 Xinference的 Python 客户端来测试 Chat API："

#: ../../source/models/model_abilities/chat.rst:146
msgid "You can find more examples of Chat API in the tutorial notebook:"
msgstr "你可以在教程笔记本中找到更多 Chat API 的示例。"

#: ../../source/models/model_abilities/chat.rst:150
msgid "Gradio Chat"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:153
msgid ""
"Learn from an example of utilizing the Chat API with the Xinference "
"Python client."
msgstr "学习如何使用 Xinference 的 Chat API 和 Python 客户端的示例。"

#: ../../source/models/model_abilities/chat.rst:159
msgid ""
"The Generate API mirrors OpenAI's legacy `Completions API "
"<https://platform.openai.com/docs/api-reference/completions/create>`__."
msgstr "Generate API 复刻了 OpenAI 的 `Completions API <https://platform.openai.com/docs/api-reference/completions/create>`__。 "

#: ../../source/models/model_abilities/chat.rst:161
msgid ""
"The difference between the Generate API and the Chat API lies primarily "
"in the form of input. Opposite to the Chat API that takes a list of "
"messages as input, the Generate API accepts a freeform text string named "
"\"prompt\"."
msgstr "Generate API 和 Chat API 之间的区别主要在于输入形式。"
"Chat API 接受一个消息列表作为输入，Generate API 接受一个名为 prompt 的自由文本字符串作为输入。"

#: ../../source/models/model_abilities/chat.rst:230
msgid "FAQ"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:233
msgid ""
"Does Xinference's LLM provide integration methods for LangChain or "
"LlamaIndex?"
msgstr "Xinference 的 LLM 是否提供与 LangChain 或 LlamaIndex 的集成方法？"

#: ../../source/models/model_abilities/chat.rst:235
msgid ""
"Yes, you can refer to the related sections in their respective official "
"Xinference documentation. Here are the links:"
msgstr "是的，你可以参考它们各自官方Xinference文档中的相关部分。以下是链接："

#: ../../source/models/model_abilities/chat.rst:237
msgid ""
"`LangChain LLMs: Xinference "
"<https://python.langchain.com/docs/integrations/llms/xinference>`__"
msgstr ""

#: ../../source/models/model_abilities/chat.rst:239
msgid ""
"`LlamaIndex LLM integrations: Xinference  "
"<https://docs.llamaindex.ai/en/stable/examples/llm/xinference_local_deployment.html>`__"
msgstr ""

