# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-12-25 17:11+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"

#: ../../source/examples/ai_podcast.rst:5
msgid "Example: AI Podcast 🎙"
msgstr "示例：智能播客 🎙"

#: ../../source/examples/ai_podcast.rst:7
msgid "**Description**:"
msgstr "描述"

#: ../../source/examples/ai_podcast.rst:9
msgid "🎙️AI Podcast - Voice Conversations with Multiple Agents on M2 Max 💻"
msgstr "🎙️AI播客 - 在M2 Max芯片上进行多智能体语音对话"

#: ../../source/examples/ai_podcast.rst:11
msgid "**Support Language** :"
msgstr "**支持语言**："

#: ../../source/examples/ai_podcast.rst:13
msgid "English (AI_Podcast.py)"
msgstr "英文对应代码文件：AI_Podcast.py"

#: ../../source/examples/ai_podcast.rst:15
msgid "Chinese (AI_Podcast_ZH.py)"
msgstr "中文对应代码文件：AI_Podcast_ZH.py"

#: ../../source/examples/ai_podcast.rst:17
msgid "**Used Technology (EN version)** :"
msgstr "英文版本涉及技术："

#: ../../source/examples/ai_podcast.rst:19
msgid ""
"@ `OpenAI <https://twitter.com/OpenAI>`_ 's `whisper "
"<https://pypi.org/project/openai-whisper/>`_"
msgstr ""
"@ `OpenAI <https://twitter.com/OpenAI>`_ `whisper <https://pypi.org/project/openai-whisper/>`_"

#: ../../source/examples/ai_podcast.rst:21
msgid ""
"@ `ggerganov <https://twitter.com/ggerganov>`_ 's `ggml "
"<https://github.com/ggerganov/ggml>`_"
msgstr ""
"@ `ggerganov <https://twitter.com/ggerganov>`_ `ggml <https://github.com/ggerganov/ggml>`_"

#: ../../source/examples/ai_podcast.rst:23
msgid ""
"@ `WizardLM_AI <https://twitter.com/WizardLM_AI>`_ 's `wizardlm v1.0 "
"<https://huggingface.co/WizardLM>`_"
msgstr ""
"@ `WizardLM_AI <https://twitter.com/WizardLM_AI>`_ `wizardlm v1.0 <https://huggingface.co/WizardLM>`_"

#: ../../source/examples/ai_podcast.rst:25/
msgid ""
"@ `lmsysorg <https://twitter.com/lmsysorg>`_ 's `vicuna v1.3 "
"<https://huggingface.co/lmsys/vicuna-7b-v1.3>`_"
msgstr ""
"@ `lmsysorg <https://twitter.com/lmsysorg>`_ `vicuna v1.3 <https://huggingface.co/lmsys/vicuna-7b-v1.3>`_"

#: ../../source/examples/ai_podcast.rst:27
msgid "@ `Xinference <https://github.com/xorbitsai/inference>`_ as a launcher"
msgstr "`Xinference <https://github.com/xorbitsai/inference>`_ 作为平台"

#: ../../source/examples/ai_podcast.rst:29
msgid "**Detailed Explanation on the Demo Functionality** :"
msgstr "**关于演示功能的详细说明**："

#: ../../source/examples/ai_podcast.rst:31
msgid ""
"Generate the Wizardlm Model and Vicuna Model when the program is "
"launching with Xorbits Inference. Initiate the Chatroom by giving the two"
" chatbot their names and telling them that there is a human user called "
"\"username\", where \"username\" is given by user's input. Initialize a "
"empty chat history for the chatroom."
msgstr ""
"启动 XInference, 部署 Wizardlm 模型和 Vicuna 模型。"
"通过为两个模型指定名称并告诉它们有一个名为“username”的人类用户来启动聊天室，其中“username”是由用户输入提供的。然后为聊天室初始化一个空的聊天历史。"

#: ../../source/examples/ai_podcast.rst:35
msgid ""
"Use Audio device to store recording into file, and transcribe the file "
"using OpenAI's Whisper to receive a human readable text as string."
msgstr ""
"使用音频设备将录音存储到文件中，然后使用OpenAI的Whisper将文件转录为人类可读的文本字符串。"

#: ../../source/examples/ai_podcast.rst:37
msgid ""
"Based on the input message string, determine which agents the user want "
"to talk to. Call the target agents and parse in the input string and chat"
" history for the model to generate."
msgstr ""
"基于输入的消息字符串，确定用户想要与哪些代理（模型）进行对话。调用这些目标代理并将用户输入字符串和聊天历史作为输入让模型去生成对应的内容。"

#: ../../source/examples/ai_podcast.rst:40
msgid ""
"When the responses are ready, use Macos's \"Say\" Command to produce "
"audio through speaker. Each agents have their own voice while speaking."
msgstr ""
"当模型的输出准备好时，使用MacOS的“Say”命令通过扬声器生成音频。每个代理在说话时都有自己的声音。"

#: ../../source/examples/ai_podcast.rst:43
msgid ""
"Store the user input and the agent response into chat history, and "
"recursively looping the program until user explicitly says words like "
"\"see you\" in their responses."
msgstr ""
"将用户输入和代理响应存储到聊天历史中，并循环递归程序，直到用户明确在其响应中说出“再见”之类的话语。"

#: ../../source/examples/ai_podcast.rst:46
msgid "**Highlight Features with Xinference** :"
msgstr "**Xinference的突出特性**："

#: ../../source/examples/ai_podcast.rst:48
msgid ""
"With Xinference's distributed system, we can easily deploy two different "
"models in the same session and in the same \"chatroom\". With enough "
"resources, the framework can deploy any amount of models you like at the "
"same time."
msgstr ""
"借助 Xinference 的分布式系统，我们可以轻松在同一会话和同一“聊天室”中部署两个不同的模型。"
"在足够的资源情况下，该框架可以同时部署任意数量的模型。"

#: ../../source/examples/ai_podcast.rst:51
msgid ""
"With Xinference, you can deploy the model easily by just adding a few "
"lines of code. For examples, for launching the vicuna model in the demo, "
"just by::"
msgstr ""
"使用 Xinference，只需添加几行代码就可以轻松部署模型。例如，在演示中启动vicuna模型，只需："

#: ../../source/examples/ai_podcast.rst:68
msgid ""
"Then, the Xinference client will handle \"target model downloading and "
"caching\", \"set up environment and process for the model\", and \"run "
"the service at selected endpoint. \" You are now ready to play with your "
"llm model."
msgstr ""
"然后，Xinference 客户端将处理“目标模型的下载和缓存”、“为模型设置环境和进程”以及“在选择的端点运行服务”。你现在已经准备好与你的 llm 模型交互。"

#: ../../source/examples/ai_podcast.rst:71
msgid "**Original Demo Video** :"
msgstr "**原始演示视频**"

#: ../../source/examples/ai_podcast.rst:73
msgid ""
"`🎙️AI Podcast - Voice Conversations with Multiple Agents on M2 Max💻🔥🤖 <"
"https://twitter.com/yichaocheng/status/1679129417778442240>`_"
msgstr ""
"`🎙️AI播客 - 在M2 Max芯片上进行多智能体语音对话💻🔥🤖 <https://twitter.com/yichaocheng/status/1679129417778442240>`_"

#: ../../source/examples/ai_podcast.rst:75
msgid "**Source Code** :"
msgstr "**源代码**："

#: ../../source/examples/ai_podcast.rst:77
msgid ""
"`AI_Podcast "
"<https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast.py>`_"
" (English Version)"
msgstr ""
"`AI播客 <https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast.py>`_（英文版）"

#: ../../source/examples/ai_podcast.rst:79
msgid ""
"`AI_Podcast_ZH "
"<https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast_ZH.py>`_"
" (Chinese Version)"
msgstr ""
"`AI播客 <https://github.com/xorbitsai/inference/blob/main/examples/AI_podcast_ZH.py>`_（中文版）"
