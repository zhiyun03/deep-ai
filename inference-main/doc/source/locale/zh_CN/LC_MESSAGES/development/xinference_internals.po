# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, Xorbits Inc.
# This file is distributed under the same license as the Xinference package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Xinference \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-05-31 11:46+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.14.0\n"

#: ../../source/development/xinference_internals.rst:3
msgid "The internals of Xinference"
msgstr "Xinference 的内部结构"

#: ../../source/development/xinference_internals.rst:6
msgid "Table of contents:"
msgstr "目录"

#: ../../source/development/xinference_internals.rst:9
msgid "Overview"
msgstr "概述"

#: ../../source/development/xinference_internals.rst:10
msgid ""
"Xinference leverages `Xoscar <https://github.com/xorbitsai/xoscar>`_, an "
"actor programming framework we designed, as its core component to manage "
"machines, devices, and model inference processes. Each actor serves as a "
"basic unit for model inference and various inference backends can be "
"integrate into the actor, enabling us to support multiple inference "
"engines and hardware. These actors are hosted and scheduled within actor "
"pools, which are designed to be asynchronous and non-blocking and "
"function as resource pools."
msgstr ""
"Xinference 利用我们设计的 actor 编程框架 `Xoscar <https://github.com/"
"xorbitsai/xoscar>`_ 作为其核心组件，以管理机器、设备和模型推理进程。每个 "
"actor 都是模型推理的基本单元，各种推理后端可以集成到 actor 中，从而使我们"
"能够支持多种推理引擎和硬件。这些 actor 在 actor 池中托管和调度，actor 池"
"具有资源池的功能，actor 的设计是异步和非阻塞的，。"

#: ../../source/development/xinference_internals.rst:22
msgid ""
"Both supervisor and worker are actor instances. Initially, an actor pool,"
" serving as a resource pool, needs to be created on each server; and each"
" actor can utilize a CPU core or a GPU device. Each server has its own "
"address (IP address or hostname), so actors on different computing nodes "
"can communicate with each other through these addresses. See `Actor`_ for"
" more information."
msgstr ""
"supervisor 和 worker 都是 actor 实例。需要先在每台服务器上创建一个作为"
"资源池的 actor 池；每个 actor 可以使用一个 CPU 内核或一块 GPU 设备。每台"
"服务器都有自己的地址（IP 地址或主机名），因此不同计算节点上的 actor 可以"
"通过这些地址相互通信。更多信息，请参阅 `Actor`_。"

#: ../../source/development/xinference_internals.rst:27
msgid "RESTful API"
msgstr "RESTful API"

#: ../../source/development/xinference_internals.rst:28
msgid ""
"The RESTful API is implemented using `FastAPI "
"<https://github.com/tiangolo/fastapi>`_, as specified in "
"`api/restful_api.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/restful_api.py>`_."
msgstr ""
"RESTful API 是利用 `FastAPI <https://github.com/tiangolo/fastapi>`_ 实现"
"的，具体代码在 `api/restful_api.py <https://github.com/xorbitsai/"
"inference/tree/main/xinference/api/restful_api.py>`_。"

#: ../../source/development/xinference_internals.rst:35
msgid ""
"This is an example of the API ``/status``, it's corresponding function is"
" ``get_status``. You can add connection between RESTful API and the "
"backend function you want in `api/restful_api.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/restful_api.py>`_."
msgstr ""
"这是一个 API 的示例，API ``/status`` 对应函数 ``get_status``。您可以在 `"
"api/restful_api.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/api/restful_api.py>`_ 中添加 RESTful API 和对应后端函数之间的"
"关系。"

#: ../../source/development/xinference_internals.rst:39
msgid "Command Line"
msgstr "命令行"

#: ../../source/development/xinference_internals.rst:40
msgid ""
"The Command Line is implemented using `Click "
"<https://click.palletsprojects.com/>`_, as specified in "
"`deploy/cmdline.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/deploy/cmdline.py>`_,"
" allowing users to interact with the Xinference deployment features "
"directly from the terminal."
msgstr ""
"命令行是通过 `Click <https://click.palletsprojects.com/>`_ 实现的，具体"
"代码在 `deploy/cmdline.py <https://github.com/xorbitsai/inference/tree/"
"main/xinference/deploy/cmdline.py>`_，命令行允许用户直接在终端与 "
"Xinference 进行交互。"

#: ../../source/development/xinference_internals.rst:45
msgid "Entry Points"
msgstr "入口点"

#: ../../source/development/xinference_internals.rst:46
msgid "Take the command-lines we implemented as examples:"
msgstr "以我们实现的命令行为例："

#: ../../source/development/xinference_internals.rst:48
msgid ""
"``xinference``: Provides commands for model management, including "
"registering/unregistering models, listing all registered/running models, "
"and launching or terminating specific models. It also features "
"interactive commands like generate and chat for testing and interacting "
"with deployed models in real-time."
msgstr ""
"``xinference``：提供命令用于模型管理，包括注册/取消注册模型、列出所有已"
"注册/运行的模型，以及启动或终止特定模型。它还提供生成语言和聊天等交互式"
"命令，用于测试或交互已部署的模型。"

#: ../../source/development/xinference_internals.rst:52
msgid "``xinference-local``: Starts a local Xinference service."
msgstr "``xinference-local``：启动一个本地 Xinference 服务。"

#: ../../source/development/xinference_internals.rst:54
msgid ""
"``xinference-supervisor``: Initiates a supervisor process that manages "
"and monitors worker actors within a distributed setup."
msgstr ""
"``xinference-supervisor``：启动 supervisor 进程，在分布式环境中管理和监控"
" worker actors。"

#: ../../source/development/xinference_internals.rst:56
msgid ""
"``xinference-worker``: Starts a worker process that executes tasks "
"assigned by the supervisor, utilizing available computational resources "
"effectively."
msgstr ""
"``xinference-worker``：启动 worker 进程，利用可用计算资源，执行 "
"supervisor 分配的任务。"

#: ../../source/development/xinference_internals.rst:59
msgid ""
"Each command is equipped with ``options`` and ``flags`` to customize its "
"behavior, such as specifying log levels, host addresses, port numbers, "
"and other relevant settings."
msgstr ""
"每条命令都配有 ``option`` 和 ``flag``，可自定义其行为，如指定日志级别、"
"主机地址、端口号和其他相关设置。"

#: ../../source/development/xinference_internals.rst:62
msgid ""
"Python projects define command-line console entry points in `setup.cfg` "
"or `setup.py`."
msgstr "Python 项目会在 `setup.cfg` 或 `setup.py` 中定义命令行控制台入口点。"

#: ../../source/development/xinference_internals.rst:72
msgid ""
"The command-line ``xinference`` can be referred to code in "
"``xinference.deploy.cmdline:cli``."
msgstr "命令行 ``xinference`` 可参考 ``xinference.deploy.cmdline:cli`` 中的代码。"

#: ../../source/development/xinference_internals.rst:75
msgid "Click"
msgstr "Click"

#: ../../source/development/xinference_internals.rst:76
msgid "We use Click to implement a specific command-line:"
msgstr "我们使用 Click 来实现特定的命令行："

#: ../../source/development/xinference_internals.rst:95
msgid ""
"For example, the ``xinference-local`` command allows you to define the "
"host address and port."
msgstr "例如，``xinference-local`` 命令允许您定义主机地址和端口。"

#: ../../source/development/xinference_internals.rst:98
msgid "Actor"
msgstr "Actor"

#: ../../source/development/xinference_internals.rst:99
msgid ""
"Xinference is fundamentally based on `Xoscar "
"<https://github.com/xorbitsai/xoscar>`_, our actor framework, which can "
"manage computational resources and Python processes to support scalable "
"and concurrent programming. The following is a pseudocode demonstrating "
"how our Worker Actor works, the actual Worker Actor is more complex than "
"this."
msgstr ""
"Xinference 以 `Xoscar <https://github.com/xorbitsai/xoscar>`_ 为基础，"
"Xoscar 是我们的 actor 框架，可以管理计算资源和 Python 进程，支持可扩展的"
"并发编程。下面的伪代码演示了 Worker Actor 的工作原理，实际的 Worker Actor"
" 要比这个复杂得多。"

#: ../../source/development/xinference_internals.rst:126
msgid ""
"We use the ``WorkerActor`` as an example to illustrate how we build the "
"Xinference. Each actor class is a standard Python class that inherits "
"from ``xoscar.Actor``. An instance of this class is a specific actor "
"within the actor pool."
msgstr ""
"我们以 ``WorkerActor`` 为例，说明如何构建 Xinference。每个 actor 类都是"
"继承自 ``xoscar.Actor`` 的标准 Python 类。该类的实例就是 actor 池中的一个"
"特定的 actor。"

#: ../../source/development/xinference_internals.rst:130
msgid ""
"**Define Actor Actions**: Each actor needs to define certain actions or "
"behaviors to accomplish specific tasks. For instance, the model inference"
" ``WorkerActor`` needs to launch the model (``launch_model``), list the "
"models in this actor (``list_models``), terminate a model "
"(``terminate_model``). There are two special methods worth noting. The "
"``__post_create__`` is invoked before the actor is created, allowing for "
"necessary initializations. The ``__pre_destroy__`` is called after the "
"actor is destroyed, allowing for cleanup or finalization tasks."
msgstr ""
"**定义 Actor 的行为**：每个 actor 都需要定义某些动作或行为来完成特定任务"
"。例如，模型推理 ``WorkerActor`` 需要启动模型（``launch_model``）、列出该"
" actor 中的模型（``list_models``）、终止模型（``termininate_model``）。有"
"两个特殊方法值得注意。``__post_create__`` 在创建 actor 之前调用，进行必要"
"的初始化。而 ``__pre_destroy__`` 会在 actor 被销毁后调用，执行清理任务。"

#: ../../source/development/xinference_internals.rst:136
msgid ""
"**Reference Actor and Invoke Methods**: When an actor is created, it "
"yields a reference variable so that other actors can reference it. The "
"actor reference can also be referenced with the address. Suppose the "
"``WorkerActor`` is created and the reference variable is ``worker_ref``,"
"  the ``launch_model`` method of this actor class can be invoked by "
"calling ``worker_ref.launch_model()``. Even if the actor's method is "
"originally a synchronized method, when called with an actor reference, it"
" will become as an asynchronous method."
msgstr ""
"**引用 Actor 和调用方法**：当创建一个 Actor 时，它会产生一个引用变量，"
"以便其他 Actor 可以引用它。Actor 也可以用 IP 地址来引用。假设创建了 ``"
"WorkerActor``，且引用变量为 ``worker_ref``，那么就可以通过调用 ``worker_"
"ref.launch_model()`` 来调用该 Actor 类的 ``launch_model``。即使 actor 中"
"的方法原来是一个传统的阻塞式的方法，当我们使用引用变量调用这个方法时，它"
"也变成了一个异步方法。"

#: ../../source/development/xinference_internals.rst:143
msgid ""
"**Inference Engine**: The actor can manage the process, and the inference"
" engine is also a process. In the launch model part of the "
"``WorkerActor``, we can initialize different inference engines according "
"to the user's need. Therefore, Xinference can support multiple inference "
"engines and can easily adapt to new inference engines in the future."
msgstr ""
"**推理引擎**：Actor 可以管理进程，而推理引擎也是一种进程。在 ``"
"WorkerActor`` 的启动模型部分，我们可以根据用户的需要初始化不同的推理引擎"
"。因此，Xinference 可以支持多种推理引擎，并能轻松适应未来的新推理引擎。"

#: ../../source/development/xinference_internals.rst:148
msgid ""
"See `Xoscar document <https://xoscar.dev/en/latest/getting_started/llm-"
"inference.html>`_ for more actor use cases."
msgstr ""
"请参阅 `Xoscar 文档 <https://xoscar.dev/en/latest/getting_started/llm-"
"inference.html>`_ 了解更多 Actor 用例。"

#: ../../source/development/xinference_internals.rst:151
msgid "Asynchronous Programming"
msgstr "异步编程"

#: ../../source/development/xinference_internals.rst:153
msgid ""
"Both Xinference and Xoscar highly utilize asynchronous programming of "
"``asyncio``. Asynchronous programming is a programming paradigm that does"
" not block. Instead, requests and function calls are issued and executed "
"in the background and results are returned in the future. This enables us"
" to perform activities concurrently."
msgstr ""
"Xinference 和 Xoscar 非常依赖异步编程库 ``asyncio``。异步编程是一种非阻塞"
"的编程范式。相比于传统的阻塞式的函数调用，异步编程中的请求或函数调用在"
"后台执行，运行结果在未来某个时刻返回。异步编程的优势是使得可以同时并发"
"进行很多不同的活动或任务。"

#: ../../source/development/xinference_internals.rst:159
msgid ""
"If you're not familiar with Pythons's ``asyncio``, you can see more "
"tutorials for help:"
msgstr "如果您不熟悉 Python 的 ``asyncio``，可以查看更多教程以获得帮助："

#: ../../source/development/xinference_internals.rst:161
msgid ""
"`Python Asyncio Tutorial <https://bbc.github.io/cloudfit-public-"
"docs/asyncio/asyncio-part-1.html>`__"
msgstr ""
"`Python Asyncio 教程 <https://bbc.github.io/cloudfit-public-docs/asyncio/"
"asyncio-part-1.html>`__"

#: ../../source/development/xinference_internals.rst:163
msgid ""
"`Real Python's asyncio Tutorial <https://realpython.com/async-io-"
"python/>`__"
msgstr "`Real Python asyncio 教程 <https://realpython.com/async-io-python/>`__"

#: ../../source/development/xinference_internals.rst:165
msgid ""
"`Python Official Documentation "
"<https://docs.python.org/3/library/asyncio.html>`__"
msgstr "`Python 官方文档 <https://docs.python.org/3/library/asyncio.html>`__"

#: ../../source/development/xinference_internals.rst:169
msgid "Model"
msgstr "模型"

#: ../../source/development/xinference_internals.rst:171
msgid ""
"Xinference supports different types of models including large language "
"models (LLMs), image models, audio models, embedding models, etc. All "
"models are implemented in `model/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model>`_."
msgstr ""
"Xinference 支持不同类型的模型，包括大型语言模型（LLM）、图像模型、音频"
"模型、嵌入模型等。所有模型在 `model/ <https://github.com/xorbitsai/"
"inference/tree/main/xinference/model>`_ 文件夹下实现。"

#: ../../source/development/xinference_internals.rst:175
msgid "LLM"
msgstr ""

#: ../../source/development/xinference_internals.rst:177
msgid ""
"Take `model/llm/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm>`_"
" for example, it focuses on the management and instantiation of LLMs. It "
"includes detailed implementations for loading, configuring, and deploying"
" LLMs."
msgstr ""
"以 `model/llm/ <https://github.com/xorbitsai/inference/tree/main/"
"xinference/model/llm>`_ 为例，它主要管理和启动 LLM，包括加载、配置和运行"
"大语言模型。"

#: ../../source/development/xinference_internals.rst:181
msgid ""
"We support many backends such as GGML, PyTorch, and vLLM. Our generated "
"content is compatible with the format of OpenAI, supporting features such"
" as streaming output and returning chat completion format (for chat "
"models only). Therefore, there is a lot of adaptation work to be done "
"after the model generate content. These tasks are not difficult, but they"
" do require some time. When writing this part of the code, please refer "
"to the `OpenAI API documentation "
"<https://platform.openai.com/docs/introduction>`_ and the documentation "
"of various inference backends, and make the necessary adaptations."
msgstr ""
"我们支持不同的推理后端，比如 GGML、PyTorch 和 vLLM。我们生成的内容与 "
"OpenAI 的格式兼容，比如支持流式输出（stream），对话模型以 chat completion"
" 格式返回。因此模型输出内容后要做很多适配工作。这些工作并不难，但需要一些"
"时间。编写这部分代码时，请参考 `OpenAI 的 API 文档 <https://platform."
"openai.com/docs/introduction>`_ 和各个推理后端的文档，做必要的适配。"

#: ../../source/development/xinference_internals.rst:185
msgid "JSON"
msgstr ""

#: ../../source/development/xinference_internals.rst:187
msgid ""
"In `model/llm/llm_family.json "
"<https://github.com/xorbitsai/inference/blob/main/xinference/model/llm/llm_family.json>`_,"
" we utilize JSON files to manage the metadata of emerging open-source "
"models. Adding a new model does not necessitate writing new code, it "
"merely requires appending new metadata to the existing JSON file."
msgstr ""
"在 `model/llm/llm_family.json <https://github.com/xorbitsai/inference/"
"blob/main/xinference/model/llm/llm_family.json>`_ 中，我们利用 JSON 文件"
"来管理新出现的开源模型的元数据。添加一个新模型并不需要编写新代码，只需要"
"将新的元数据添加到现有的 JSON 文件中即可。"

#: ../../source/development/xinference_internals.rst:214
msgid ""
"This is an example of how to define the Llama-2 chat model. The "
"``model_specs`` define the information of the model, as one model family "
"usually comes with various sizes, quantization methods, and file formats."
" For instance, the ``model_format`` could be ``pytorch`` (using Hugging "
"Face Transformers or vLLM as backend), ``ggmlv3`` (a tensor library "
"associated with llama.cpp), or ``gptq`` (a post-training quantization "
"framework). The ``model_id`` defines the repository of the model hub from"
" which Xinference downloads the checkpoint files. Furthermore, due to "
"distinct instruction-tuning processes, different model families have "
"varying prompt styles. The ``prompt_style`` in the JSON file specifies "
"how to format prompts for this particular model. For example, "
"``system_prompt`` and ``roles`` are used to specify the instructions and "
"personality of the model."
msgstr ""
"这是一个如何定义 Llama-2 聊天模型的示例。``model_specs`` 定义了模型的信息"
"，因为一个模型系列通常有不同的尺寸、量化方法和文件格式。例如，``model_"
"format`` 可以是 ``pytorch`` （使用 Hugging Face Transformers 或 vLLM 作为"
"后端）、 ``ggmlv3`` （与 llama.cpp 相关的张量库）或 ``gptq`` （训练后量化"
"框架）。 ``model_id`` 定义了模型中心的资源库，Xinference 从模型中心下载"
"检查点文件。此外，由于不同的指令调整过程，不同的模型系列有不同的提示风格"
"。JSON 文件中的 ``prompt_style`` 指定了该特定模型的提示格式。例如，``"
"system_prompt`` 和 ``roles`` 用于指定模型的指令和个性。"

#: ../../source/development/xinference_internals.rst:224
msgid "Code Walkthrough"
msgstr "代码指南"

#: ../../source/development/xinference_internals.rst:226
msgid ""
"The main code is located in the `xinference/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference>`_:"
msgstr ""
"主要代码位于 `xinference/ <https://github.com/xorbitsai/inference/tree/"
"main/xinference>`_："

#: ../../source/development/xinference_internals.rst:228
msgid ""
"`api/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api>`_: "
"`restful_api.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/restful_api.py>`_"
" is the core part that sets up and runs the RESTful APIs. It integrates "
"an authentication service (the specific code is located in `oauth2/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/api/oauth2>`_),"
" as some or all endpointsrequire user authentication."
msgstr ""
"`api/ <https://github.com/xorbitsai/inference/tree/main/xinference/api>`_"
"：`restful_api.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/api/restful_api.py>`_ 是设置和运行 RESTful API 的核心部分。它"
"集成了一个身份验证服务（具体代码位于 `oauth2/ <https://github.com/"
"xorbitsai/inference/tree/main/xinference/api/oauth2>`_），因为部分或所有"
"端口需要用户身份验证。"

#: ../../source/development/xinference_internals.rst:233
msgid ""
"`client/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/client>`_: "
"This is the client of Xinference."
msgstr ""
"`client/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"client>`_：这是 Xinference 的客户端。"

#: ../../source/development/xinference_internals.rst:235
msgid ""
"`oscar/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/client/oscar>`_"
" defines the Actor Client which acts as a client interface for "
"interacting with models deployed in a Xinference cluster."
msgstr ""
"`oscar/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"client/oscar>`_ 定义了 Actor 客户端，它是一个客户端接口，用于与 "
"Xinference 中的模型交互。"

#: ../../source/development/xinference_internals.rst:238
msgid ""
"`restful/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/client/restful>`_"
" implements a RESTful client for interacting with a Xinference service."
msgstr ""
"`restful/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"client/restful>`_ 实现与 Xinference 服务交互的 RESTful 客户端。"

#: ../../source/development/xinference_internals.rst:241
msgid ""
"`core/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core>`_: "
"This is the core part of Xinference."
msgstr ""
"`core/ <https://github.com/xorbitsai/inference/tree/main/xinference/core>"
"`_：这是 Xinference 的核心部分。"

#: ../../source/development/xinference_internals.rst:243
msgid ""
"`metrics.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/metrics.py>`_"
" and `resource.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/resource.py>`_"
" defines a set of tools for collecting and reporting metrics and the "
"status of node resources, including model throughput, latency, the usage "
"of CPU and GPU, memory usage, and more."
msgstr ""
"`metrics.py <https://github.com/xorbitsai/inference/tree/main/xinference/"
"core/metrics.py>`_ 和 `resource.py <https://github.com/xorbitsai/"
"inference/tree/main/xinference/core/resource.py>`_ 定义了一套用于收集和"
"报告指标以及节点资源状态的工具，包括模型吞吐量、延迟、CPU 和 GPU 的使用率"
"、内存使用率等。"

#: ../../source/development/xinference_internals.rst:248
msgid ""
"`image_interface.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/image_interface.py>`_"
" and `chat_interface.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/chat_interface.py>`_"
" implement `Gradio <https://github.com/gradio-app/gradio>`_ interfaces "
"for image and chat models, respectively. These interfaces allow users to "
"interact with models through a Web UI, such as generating images or "
"engaging in chat. They build user interfaces using the gradio package and"
" communicate with backend models through our RESTful APIs."
msgstr ""
"`image_interface.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/core/image_interface.py>`_ 和 `chat_interface.py <https://"
"github.com/xorbitsai/inference/tree/main/xinference/core/chat_interface."
"py>`_ 分别为图像和聊天模型实现了 `Gradio <https://github.com/gradio-app/"
"gradio>`_ 接口。这些接口允许用户通过 Web 界面与模型进行交互，例如生成图像"
"或进行聊天。代码使用 Gradio 软件包构建用户界面，并通过我们的 RESTful API "
"与后端模型通信。"

#: ../../source/development/xinference_internals.rst:254
msgid ""
"`worker.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/worker.py>`_"
" and `supervisor.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/supervisor.py>`_"
" respectively define the logic for worker actors and supervisor actor. "
"Worker actors are responsible for carrying out specific model computation"
" tasks, while supervisor actors manage the lifecycle of worker nodes, "
"schedule tasks, and monitor system states."
msgstr ""
"`worker.py <https://github.com/xorbitsai/inference/tree/main/xinference/"
"core/worker.py>`_ 和 `supervisor.py <https://github.com/xorbitsai/"
"inference/tree/main/xinference/core/supervisor.py>`_ 分别定义了 worker "
"actor 和 supervisor actor 的逻辑。worker actor 负责执行特定的模型计算任务"
"，而 supervisor actor 则管理 Worker 节点的生命周期和任务调度，并监控系统"
"状态。"

#: ../../source/development/xinference_internals.rst:259
msgid ""
"`status_guard.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/status_guard.py>`_"
" implements a status monitor to track the status of models (like "
"creating, updating, terminating, etc.). It allows querying status "
"information of model instances and managing these statuses based on the "
"model's UID."
msgstr ""
"`status_guard.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/core/status_guard.py>`_ 实现了一个状态监视器，用于跟踪模型的"
"状态（如创建、更新、终止等）。它允许根据模型的 UID 查询模型实例的状态信息"

#: ../../source/development/xinference_internals.rst:263
msgid ""
"`cache_tracker.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/cache_tracker.py>`_"
" defines a cache tracker for recording and managing cache status and "
"information of model versions. It supports recording cache locations and "
"statuses of model versions and querying model version information based "
"on model names."
msgstr ""
"`cache_tracker.py <https://github.com/xorbitsai/inference/tree/main/"
"xinference/core/cache_tracker.py>`_ 定义了一个缓存跟踪器，用于记录和管理"
"缓存状态和模型版本信息。它支持记录缓存位置和模型版本的状态，并根据模型"
"名称查询模型版本信息。"

#: ../../source/development/xinference_internals.rst:267
msgid ""
"`event.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/event.py>`_"
" defines an event collector for gathering and reporting various runtime "
"events of models, such as information, warnings, and errors. `model.py "
"<https://github.com/xorbitsai/inference/tree/main/xinference/core/model.py>`_"
" defines a Model Actor, the core component for direct model interactions."
" The Model Actor is responsible for executing model inference requests, "
"handling input and output data streams, and supports various types of "
"model operations."
msgstr ""
"`event.py <https://github.com/xorbitsai/inference/tree/main/xinference/"
"core/event.py>`_ 定义了一个事件收集器，用于收集和报告各种运行时模型的事件"
"，如信息、警告和错误。`model.py <https://github.com/xorbitsai/inference/"
"tree/main/xinference/core/model.py>`_ 定义了一个模型 Actor，它是与模型"
"直接交互的核心组件。模型 actor 负责执行模型推理请求，处理输入和输出数据流"
"，并支持各种模型操作。这两个部分都使用 `Xoscar <https://github.com/"
"xorbitsai/xoscar>`_ 用于并发和分布式执行。"

#: ../../source/development/xinference_internals.rst:273
msgid ""
"`deploy/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/deploy>`_: "
"It provides a command-line interface (CLI) for interacting with the "
"Xinference framework, allowing users to perform operations by command "
"line. See `Command Line`_ for more information."
msgstr ""
"`deploy/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"deploy>`_：它提供了一个命令行界面（CLI），用于与 Xinference 框架进行交互"
"，允许用户通过命令行进行操作。更多信息，请参见 `Command Line`_。"

#: ../../source/development/xinference_internals.rst:276
msgid ""
"`locale/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/locale>`_: "
"It supports multi-language localization. By simply adding and updating "
"JSON translation files, it becomes possible to support more languages, "
"improving user experience."
msgstr ""
"`locale/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"locale>`_：它支持多语言本地化。只需添加和更新 JSON 翻译文件，就可以支持更"
"多语言，改善用户体验。"

#: ../../source/development/xinference_internals.rst:279
msgid ""
"`model/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/model>`_: It"
" provides a structure for model descriptions, creation, and caching. See "
"`Model`_ for more information."
msgstr ""
"`model/ <https://github.com/xorbitsai/inference/tree/main/xinference/"
"model>`_：它为模型描述、创建和缓存提供了一个框架。请参见 `Model`_ 以获取"
"更多信息。"

#: ../../source/development/xinference_internals.rst:282
msgid ""
"`web/ui/ "
"<https://github.com/xorbitsai/inference/tree/main/xinference/web/ui>`_: "
"The js code of the frontend (Web UI)."
msgstr ""
"`web/ui/ <https://github.com/xorbitsai/inference/tree/main/xinference/web"
"/ui>`_：前端（用户界面）的js代码。"

#~ msgid ""
#~ "Xinference supports different types of "
#~ "models including large language models "
#~ "(LLMs), image models, audio models, "
#~ "embedding models, etc"
#~ msgstr ""
#~ "Xinference 支持不同类型的模型，包括大型"
#~ "语言模型（LLM）、图像模型、音频模型、"
#~ "嵌入模型等。所有模型都在 `model/ "
#~ "<https://github.com/xorbitsai/"
#~ "inference/tree/main/xinference/model>`"
#~ "_ 中实现。以 `llm/ <https:/"
#~ "/github.com/xorbitsai/inference/tree/"
#~ "main/xinference/model/llm>`_ 为例"
#~ "，它侧重于 LLM 的管理和实例化。它"
#~ "包括加载、配置和部署 LLM 的详细实现"
#~ "，包括处理不同类型的量化和模型格式。"
#~ "在 `llm/ <https://github.com"
#~ "/xorbitsai/inference/tree/main/xinference"
#~ "/model/llm>`_ 中，它支持许多后"
#~ "端，如 `GGML <https://github."
#~ "com/xorbitsai/inference/tree/main/"
#~ "xinference/model/llm/ggml>`_、`"
#~ "PyTorch <https://github.com/xorbitsai"
#~ "/inference/tree/main/xinference/model/"
#~ "llm/pytorch>`_、`SGLang <https:"
#~ "//github.com/xorbitsai/inference/tree"
#~ "/main/xinference/model/llm/sglang>`"
#~ "_ 和 `vLLM <https://github."
#~ "com/xorbitsai/inference/tree/main/"
#~ "xinference/model/llm/vllm>`_。"

#~ msgid ""
#~ "Take `llm/ "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm>`_"
#~ " for example, it focuses on the "
#~ "management and instantiation of LLMs. It"
#~ " includes detailed implementations for "
#~ "loading, configuring, and deploying LLMs, "
#~ "including handling different types of "
#~ "quantization and model formats. In `llm/"
#~ " "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm>`_,"
#~ " it supports many backends such as"
#~ " `GGML "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/ggml>`_,"
#~ " `PyTorch "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/pytorch>`_,"
#~ " `SGLang "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/sglang>`_"
#~ " and `vLLM "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model/llm/vllm>`_."
#~ msgstr ""
#~ "所有模型都在 `model/ <https://"
#~ "github.com/xorbitsai/inference/tree/main"
#~ "/xinference/model>`_ 中实现。以 "
#~ "`llm/ <https://github.com/"
#~ "xorbitsai/inference/tree/main/xinference/"
#~ "model/llm>`_ 为例，它侧重于 LLM"
#~ " 的管理和实例化。它包括加载、配置"
#~ "和部署 LLM 的详细实现，包括处理不同"
#~ "类型的量化和模型格式。在 `llm/ "
#~ "<https://github.com/xorbitsai/"
#~ "inference/tree/main/xinference/model/llm"
#~ ">`_ 中，它支持许多后端，如 `"
#~ "GGML <https://github.com/xorbitsai"
#~ "/inference/tree/main/xinference/model/"
#~ "llm/ggml>`_、`PyTorch <https:/"
#~ "/github.com/xorbitsai/inference/tree/"
#~ "main/xinference/model/llm/pytorch>`_"
#~ "、`SGLang <https://github.com/"
#~ "xorbitsai/inference/tree/main/xinference/"
#~ "model/llm/sglang>`_ 和 `vLLM "
#~ "<https://github.com/xorbitsai/"
#~ "inference/tree/main/xinference/model/llm"
#~ "/vllm>`_。"

#~ msgid ""
#~ "All models are implemented in `model/"
#~ " "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/model>`_."
#~ msgstr ""
#~ "所有模型在 `model/ <https://"
#~ "github.com/xorbitsai/inference/tree/main"
#~ "/xinference/model>`_ 中实现"

#~ msgid ""
#~ "In ``model/llm/``, it supports many "
#~ "backends such as GGML, PyTorch, and "
#~ "vLLM."
#~ msgstr "在 ``model/llm/`` 下，我们支持了很多后端，比如 GGML、PyTorch、和 vLLM。"

#~ msgid ""
#~ "`oscar/ "
#~ "<https://github.com/xorbitsai/inference/tree/main/xinference/client/oscar>`_"
#~ " defines the Actor Client which acts"
#~ " as a client interface for "
#~ "interacting with models deployed in a"
#~ " server environment. It includes "
#~ "functionalities to register/unregister models, "
#~ "launch/terminate models, and interact with "
#~ "different types of models. This part "
#~ "heavily utilizes ``asyncio`` for asynchronous"
#~ " operations. See `Concurrency`_ for more"
#~ " information."
#~ msgstr ""
#~ "`oscar/ <https://github.com/"
#~ "xorbitsai/inference/tree/main/xinference/"
#~ "client/oscar>`_ 定义了 Actor "
#~ "客户端，它是一个客户端接口，用于与部署"
#~ "在服务器环境中的模型交互。它包括注册/"
#~ "取消注册模型、启动/终止模型，以及与"
#~ "不同类型的模型交互的功能。这部分主要使用"
#~ " ``asyncio`` 进行异步操作。更多"
#~ "信息，请参见 `Concurrency`_。"

#~ msgid "Concurrency"
#~ msgstr "并发性"

#~ msgid ""
#~ "Both Xinference and Xoscar highly "
#~ "utilize coroutine programming of ``asyncio``."
#~ msgstr "Xinference 和 Xoscar 都高度利用了 ``asyncio`` 进行协程编程。"
